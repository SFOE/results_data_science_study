{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import helpers\n",
    "import numpy as np\n",
    "from settings import data_folder,preprocessed_folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From regbl, load link between ID building and ID of the municipality number (BFS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfs_number = pd.read_csv(\n",
    "    os.path.join(data_folder, \"ch\", \"gebaeude_batiment_edificio.csv\"),\n",
    "    sep=\"\\t\",\n",
    "    usecols=[0, 2, 3],\n",
    "    dtype={\"EGID\": \"category\", \"GGDENR\": \"category\", \"GGDENAME\": \"category\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load also mapping between postcode and BFS number. The mapping is not perfect, since we do not have a perfect matching, but we take the number that is more represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_plz_bfs = (\n",
    "    pd.read_excel(os.path.join(data_folder, \"mapping_plz_bfs.xlsx\"), sheet_name=\"PLZ4\")\n",
    "    .rename(columns={\"PLZ4\": \"PLZ\", \"KTKZ\": \"GGDENR\", \"GDENAMK\": \"GGDENAME\"})\n",
    "    .astype({\"PLZ\": \"category\", \"GGDENR\": str, \"GGDENAME\": \"category\"})\n",
    "    .astype({\"GGDENR\": \"category\"})\n",
    ")\n",
    "# Find the most representative municipality\n",
    "map_plz_bfs = map_plz_bfs.loc[map_plz_bfs.groupby(\"PLZ\")[\"%_IN_GDE\"].idxmax()]\n",
    "map_plz_bfs = map_plz_bfs.drop(columns=[\"%_IN_GDE\", \"GDENR\"])\n",
    "map_plz_bfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the SFOE building renovation database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = helpers.load_database_init()\n",
    "# Consider only payments that are labelled as \"FIX\"\n",
    "db = db[db[\"Status\"] == \"FIX\"]\n",
    "db = db.drop(columns=[\"Status\"])\n",
    "# Doesn't match any number in Regbl\n",
    "to_replace = {\n",
    "    \"EGID\": {\n",
    "        \"999\": np.nan,\n",
    "        \"99999\": np.nan,\n",
    "        \"9999999\": np.nan,\n",
    "        \"99999999\": np.nan,\n",
    "        \"999999999\": np.nan,\n",
    "        \"-99\": np.nan,\n",
    "        \"1\": np.nan,\n",
    "    }\n",
    "}\n",
    "db = db.replace(to_replace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First use the RegBL to obtain the BFS number\n",
    "tmp = pd.merge(db, bfs_number, on=\"EGID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which building EGID is not present in the RegBL and that do not contain \";\" (corresponds to several buildings separeted with ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"How many buildings do not have any EGID in the database ?\",len(db[db[\"EGID\"].isnull()]))\n",
    "# Uncomment to see which\n",
    "# db_no_missing = db[~db.EGID.isna()]\n",
    "# db_no_missing[\n",
    "#     (~db_no_missing.EGID.isin(bfs_number.EGID))\n",
    "#     & (~db_no_missing.EGID.str.contains(\";\"))\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For missing values, fill using the mapping between the postal address and the BFS number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tmp.GGDENR.isna()\n",
    "add_postal_code = pd.merge(\n",
    "    tmp[mask].drop(columns=[\"GGDENR\", \"GGDENAME\"]),\n",
    "    map_plz_bfs,\n",
    "    on=\"PLZ\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "tmp = pd.concat(\n",
    "    (\n",
    "        tmp[~mask],\n",
    "        add_postal_code,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the typology of the municipality and whether the municipality is considered to be alpine or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map BFS number (GGDENR) to urban/rural/midland/alpine regions\n",
    "path = os.path.join(data_folder, \"alpin.xlsx\")\n",
    "renaming = {\n",
    "    \"Alpine\": {\n",
    "        \"Communes hors des régions de montagne\": \"No\",\n",
    "        \"Communes des régions de montagne\": \"Yes\",\n",
    "    }\n",
    "}\n",
    "alpine = (\n",
    "    helpers.read_xlsx_from_atlas(path, nrows=2212)\n",
    "    .rename(columns={\"Classification des communes\": \"Alpine\"})\n",
    "    .replace(renaming)\n",
    ")\n",
    "\n",
    "path = os.path.join(data_folder, \"urbain.xlsx\")\n",
    "renaming = {\n",
    "    \"Typology\": {\n",
    "        \"Rural (3)\": \"Rural\",\n",
    "        \"Intermédiaire (2)\": \"Intermediate\",\n",
    "        \"Urbain (1)\": \"Urban\",\n",
    "    }\n",
    "}\n",
    "typology = (\n",
    "    helpers.read_xlsx_from_atlas(path, nrows=2255)\n",
    "    .rename(columns={\"Catégories\": \"Typology\"})\n",
    "    .replace(renaming)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first we need to handle the fusion of the municipalities...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If two municipalities are alpine and non-alpine, the fusion of the two municipalitiy is considered to be alpine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_alpin(to_, from_):\n",
    "    return \"No\"\n",
    "\n",
    "# Aggregate data using the new municipalities\n",
    "alpine = helpers.mapping_com(alpine, merging_alpin).drop(columns=\"Regionsname\")\n",
    "alpine.to_csv(os.path.join(preprocessed_folder, \"alpin_fusion_com.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the typologies. If two municipalities are urban and intermediate, the fusion of the two municipalitiy is considered to be urban. Similarly, if two municipalities are rural and intermediate, the fusion of the two municipalitiy is considered to be intermediate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_typology(to_, from_):\n",
    "    old_typ = from_[\"Typology\"]\n",
    "    new_typ = to_[\"Typology\"]\n",
    "    if old_typ == \"Urban\" or new_typ == \"Urban\":\n",
    "        return \"Urban\"\n",
    "    elif old_typ == \"Intermediate\" or new_typ == \"Intermediate\":\n",
    "        return \"Intermediate\"\n",
    "    else:\n",
    "        raise ValueError((old_typ, new_typ))\n",
    "\n",
    "\n",
    "typology = helpers.mapping_com(typology, merging_typology).drop(columns=\"Regionsname\")\n",
    "typology.to_csv(os.path.join(preprocessed_folder, \"typology_fusion_com.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_terrain = pd.merge(\n",
    "    typology, alpine, right_index=True, left_index=True\n",
    ").reset_index()\n",
    "class_terrain[\"Regions-ID\"] = class_terrain[\"Regions-ID\"].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually tracking the municipalities that are merged together\n",
    "# dict-like file indicating the changes in the GDE code of newly merged municipalities\n",
    "mapping_commune = (\n",
    "    pd.read_csv(os.path.join(data_folder, \"mapping_commune.csv\"))\n",
    "    .astype({\"From\": \"category\", \"To\": \"category\"})\n",
    "    .set_index(\"From\")\n",
    "    .to_dict()[\"To\"]\n",
    ")\n",
    "tmp.GGDENR = tmp.GGDENR.astype(\"int\").replace(mapping_commune).astype(\"category\")\n",
    "db_with_terrain_class = pd.merge(\n",
    "    tmp, class_terrain, left_on=\"GGDENR\", right_on=\"Regions-ID\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to data folder\n",
    "db_with_terrain_class.to_csv(\n",
    "    os.path.join(preprocessed_folder, \"db_with_terrain_class.csv\"), index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the socio-economic features to take into account fusion of municipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socio_features = pd.read_csv(os.path.join(data_folder, \"combined.csv\"))\n",
    "# Remove CH\n",
    "socio_features = socio_features[socio_features[\"BFS_NUMMER\"] != \"CH\"]\n",
    "socio_features = socio_features.astype({\"BFS_NUMMER\": int})\n",
    "# Replace number with the newest number\n",
    "socio_features = socio_features.replace({\"BFS_NUMMER\": mapping_commune})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge by BFS_NUMMER\n",
    "socio_features_commune_updated = helpers.combined_rows_db(\n",
    "    socio_features, col=\"BFS_NUMMER\"\n",
    ").drop(\n",
    "    columns=[\n",
    "        \"Revenu_nb_contribuable\",\n",
    "        \"Revenu_nb_habitant\",\n",
    "        \"hab_old\",\n",
    "        \"surf_hab_old\",\n",
    "        \"surf_agr_old\",\n",
    "    ]\n",
    ")\n",
    "socio_features_commune_updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socio_features_commune_updated.to_csv(\n",
    "    os.path.join(preprocessed_folder, \"socio_economic.csv\"), index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation RegBL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = helpers.get_code_translation_regbl()\n",
    "regbl = helpers.prepare_regbl()\n",
    "# Removed destroyed buildings\n",
    "regbl = regbl[(regbl[\"Annee_destr\"].isnull())].copy()\n",
    "regbl.drop(columns=[\"Annee_destr\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename codes in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = {\"EGID\": int, \"WSTWK\": \"category\", \"WSTAT\": \"category\"}\n",
    "# Load RegBL\n",
    "hab = pd.read_csv(\n",
    "    os.path.join(data_folder, \"ch\", \"wohnung_logement_abitazione.csv\"),\n",
    "    sep=\"\\t\",\n",
    "    dtype=dtype,\n",
    "    usecols=[\"EGID\", \"WSTWK\", \"WMEHRG\", \"WSTAT\", \"WAREA\", \"WAZIM\", \"WKCHE\"],\n",
    ")\n",
    "# Replace codes\n",
    "for x in hab.columns:\n",
    "    try:\n",
    "        rename_codes = name.xs(x, level=1, drop_level=True).CODTXTKF.to_dict()\n",
    "    except KeyError:\n",
    "        continue\n",
    "    hab.replace({x: rename_codes}, inplace=True)\n",
    "hab = hab[hab.WSTAT == \"existant\"].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get superficie and nb of rooms/EGID\n",
    "nb_rooms = (\n",
    "    pd.merge(regbl, hab, on=\"EGID\", how=\"left\")\n",
    "    .groupby(\"EGID\")\n",
    "    .agg({\"WAREA\": \"sum\", \"WAZIM\": \"sum\"})\n",
    "    .reset_index()\n",
    ")\n",
    "regbl = pd.merge(\n",
    "    regbl,\n",
    "    nb_rooms,\n",
    "    on=\"EGID\",\n",
    "    how=\"left\",\n",
    ")\n",
    "regbl.loc[regbl.WAREA == 0, \"WAREA\"] = np.nan\n",
    "regbl.loc[regbl.WAZIM == 0, \"WAZIM\"] = np.nan\n",
    "# Save file\n",
    "regbl.reset_index(drop=True).to_pickle(os.path.join(preprocessed_folder, \"rebgl.pickle\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97e1d25581a04eac12591ae2002b5202d82171b363b803c6e671956903272463"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
